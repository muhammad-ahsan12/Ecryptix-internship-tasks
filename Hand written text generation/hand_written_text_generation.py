# -*- coding: utf-8 -*-
"""Hand written text generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zpYvJMQ4H-Jy0OdNnMjn39MHDPaow_qI
"""

!pip install tesseract
!pip install easyocr

import zipfile
zipfile.ZipFile('/content/images.zip', 'r')
zipfile.ZipFile('/content/images.zip', 'r').extractall()
zipfile.ZipFile('/content/images.zip', 'r').close()

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.models import Sequential
import easyocr

tf.config.run_functions_eagerly(True)

# Function to load images and extract text using EasyOCR
def load_and_extract_text(image_folder):
    reader = easyocr.Reader(['en'])
    texts = []

    for filename in os.listdir(image_folder):
        if filename.endswith(".png") or filename.endswith(".jpg"):
            image_path = os.path.join(image_folder, filename)
            result = reader.readtext(image_path, detail=0)
            if result:
                texts.append(" ".join(result))

    return texts

image_folder = '/content/images'  # Replace with your image folder path
texts = load_and_extract_text(image_folder)
if not texts:
    raise ValueError("No text data was extracted from the images.")

texts

# Create character-level vocabulary
vocab = sorted(set("".join(texts)))
char2idx = {c: i for i, c in enumerate(vocab)}
idx2char = np.array(vocab)

idx2char

# Convert texts to sequences of integers
def text_to_int(text):
    return np.array([char2idx[c] for c in text])

# Convert all texts to integer sequences
sequences = [text_to_int(text) for text in texts]

if not sequences:
    raise ValueError("Text sequences are empty.")

print(f"Sample sequence: {sequences[0]}")  # Debugging line

# Flatten sequences and create input-output pairs for training
input_text = np.concatenate(sequences[:-1])
target_text = np.concatenate(sequences[1:])

if input_text.size == 0 or target_text.size == 0:
    raise ValueError("Flattened input or target text arrays are empty.")

print(input_text)
print(target_text)

# Reshape to (batch_size, sequence_length)
seq_length = 100  # Set sequence length
examples_per_epoch = len(input_text) // seq_length
input_text = input_text[:examples_per_epoch * seq_length]
target_text = target_text[:examples_per_epoch * seq_length]

if input_text.size == 0 or target_text.size == 0:
    raise ValueError("Reshaped input or target text arrays are empty.")

print(input_text)
print(target_text)

input_text = input_text.reshape((examples_per_epoch, seq_length))
target_text = target_text.reshape((examples_per_epoch, seq_length))

print(f"Reshaped input text: {input_text}")
print(f"Reshaped target text: {target_text}")

import numpy as np
import tensorflow as tf

# Generate synthetic text data
num_samples = 1000  # Number of samples
seq_length = 100     # Sequence length for each sample
vocab_size = 26      # Vocabulary size (letters a-z)

# Create random sequences of integers (simulating character indices)
input_text = np.random.randint(vocab_size, size=(num_samples, seq_length))
target_text = np.roll(input_text, shift=-1, axis=1)  # Shift input sequence to create target sequence

# Create TensorFlow dataset
dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text))
dataset = dataset.shuffle(10000).batch(64, drop_remainder=True)

# Verify dataset size
dataset_size = len(list(dataset))
print(f"Dataset size: {dataset_size}")

# Verify dataset content by printing out a few examples
for i, (input_example, target_example) in enumerate(dataset.take(1)):
    print(f"Batch {i} - Input shape: {input_example.shape}, Target shape: {target_example.shape}")
    print(f"Input example:\n{input_example.numpy()}")
    print(f"Target example:\n{target_example.numpy()}")

# Define the RNN model
vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024

model = Sequential([
    Embedding(vocab_size, embedding_dim, batch_input_shape=[64, None]),
    SimpleRNN(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
    Dense(vocab_size)
])

model.summary()

def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)

epochs = 10
history = model.fit(dataset, epochs=epochs)

